
[+] General Parameters:
LETOR 4.0 dataset: No
Training data:	../data/MQ2008/Fold1/train.txt
Test data:	../data/MQ2008/Fold1/test.txt
Validation data:	../data/MQ2008/Fold1/vali.txt
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	ERR@10
Highest relevance label (to compute ERR): 4
Feature normalization: No
Model file: ../results/ranklib_randomforest_model

[+] LambdaMART's Parameters:
No. of trees: 1000
No. of leaves: 10
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [../data/MQ2008/Fold1/train.txt]: 0... Reading feature file [../data/MQ2008/Fold1/train.txt]... [Done.]
(471 ranked lists, 9630 entries read)
Reading feature file [../data/MQ2008/Fold1/vali.txt]: 0... Reading feature file [../data/MQ2008/Fold1/vali.txt]... [Done.]
(157 ranked lists, 2707 entries read)
Reading feature file [../data/MQ2008/Fold1/test.txt]: 0... Reading feature file [../data/MQ2008/Fold1/test.txt]... [Done.]
(156 ranked lists, 2874 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V |
---------------------------------
1       | 0.4908    | 0.5351    |
2       | 0.4949    | 0.5431    |
3       | 0.4923    | 0.5476    |
4       | 0.4927    | 0.5462    |
5       | 0.4948    | 0.5455    |
6       | 0.4927    | 0.5435    |
7       | 0.4929    | 0.5403    |
8       | 0.4943    | 0.5435    |
9       | 0.494     | 0.5432    |
10      | 0.4943    | 0.5449    |
11      | 0.4981    | 0.5424    |
12      | 0.5013    | 0.5449    |
13      | 0.5042    | 0.5459    |
14      | 0.5046    | 0.5421    |
15      | 0.5063    | 0.5416    |
16      | 0.5104    | 0.5413    |
17      | 0.5098    | 0.5451    |
18      | 0.5135    | 0.5443    |
19      | 0.5127    | 0.5468    |
20      | 0.5151    | 0.5449    |
21      | 0.5169    | 0.5453    |
22      | 0.5184    | 0.5433    |
23      | 0.5183    | 0.545     |
24      | 0.5203    | 0.5418    |
25      | 0.5212    | 0.5418    |
26      | 0.5211    | 0.5442    |
27      | 0.5222    | 0.5417    |
28      | 0.5236    | 0.5429    |
29      | 0.5251    | 0.5464    |
30      | 0.5259    | 0.542     |
31      | 0.5269    | 0.5427    |
32      | 0.5279    | 0.5432    |
33      | 0.5303    | 0.5472    |
34      | 0.5322    | 0.5477    |
35      | 0.5328    | 0.5467    |
36      | 0.5309    | 0.5459    |
37      | 0.5327    | 0.5456    |
38      | 0.5348    | 0.5465    |
39      | 0.5353    | 0.5465    |
40      | 0.5348    | 0.5441    |
41      | 0.537     | 0.5443    |
42      | 0.5362    | 0.5414    |
43      | 0.538     | 0.5423    |
44      | 0.5399    | 0.5401    |
45      | 0.5405    | 0.5388    |
46      | 0.5412    | 0.5396    |
47      | 0.5434    | 0.5399    |
48      | 0.5442    | 0.5424    |
49      | 0.5452    | 0.5433    |
50      | 0.5459    | 0.5424    |
51      | 0.5456    | 0.5413    |
52      | 0.5459    | 0.5408    |
53      | 0.5471    | 0.5414    |
54      | 0.5482    | 0.5429    |
55      | 0.5479    | 0.5399    |
56      | 0.5496    | 0.5397    |
57      | 0.5514    | 0.5397    |
58      | 0.5522    | 0.5402    |
59      | 0.5526    | 0.538     |
60      | 0.5537    | 0.5383    |
61      | 0.5549    | 0.5382    |
62      | 0.5552    | 0.5391    |
63      | 0.5563    | 0.5371    |
64      | 0.5573    | 0.5374    |
65      | 0.5593    | 0.5379    |
66      | 0.5591    | 0.5376    |
67      | 0.5594    | 0.5368    |
68      | 0.5614    | 0.5372    |
69      | 0.5623    | 0.5404    |
70      | 0.5634    | 0.5416    |
71      | 0.5643    | 0.5404    |
72      | 0.5637    | 0.5404    |
73      | 0.5646    | 0.5387    |
74      | 0.5655    | 0.5402    |
75      | 0.5659    | 0.5394    |
76      | 0.5673    | 0.5398    |
77      | 0.567     | 0.5392    |
78      | 0.5676    | 0.5387    |
79      | 0.5694    | 0.5391    |
80      | 0.5697    | 0.5384    |
81      | 0.5707    | 0.5392    |
82      | 0.5731    | 0.538     |
83      | 0.5733    | 0.5376    |
84      | 0.5741    | 0.536     |
85      | 0.5743    | 0.5353    |
86      | 0.5738    | 0.535     |
87      | 0.573     | 0.5374    |
88      | 0.5751    | 0.5369    |
89      | 0.5763    | 0.539     |
90      | 0.5761    | 0.5376    |
91      | 0.5769    | 0.5374    |
92      | 0.5795    | 0.539     |
93      | 0.5798    | 0.5397    |
94      | 0.5781    | 0.5403    |
95      | 0.5783    | 0.5397    |
96      | 0.5811    | 0.54      |
97      | 0.5811    | 0.54      |
98      | 0.5817    | 0.5391    |
99      | 0.5812    | 0.5392    |
100     | 0.5817    | 0.5379    |
101     | 0.5834    | 0.5389    |
102     | 0.5837    | 0.5392    |
103     | 0.5857    | 0.5401    |
104     | 0.5865    | 0.5384    |
105     | 0.5875    | 0.5384    |
106     | 0.5896    | 0.537     |
107     | 0.59      | 0.5368    |
108     | 0.5911    | 0.5397    |
109     | 0.5912    | 0.5419    |
110     | 0.5913    | 0.5417    |
111     | 0.5924    | 0.541     |
112     | 0.593     | 0.5426    |
113     | 0.5929    | 0.5432    |
114     | 0.5942    | 0.5402    |
115     | 0.5947    | 0.5402    |
116     | 0.5948    | 0.5391    |
117     | 0.5949    | 0.5389    |
118     | 0.5966    | 0.5391    |
119     | 0.5964    | 0.5406    |
120     | 0.5988    | 0.5404    |
121     | 0.5988    | 0.5401    |
122     | 0.5987    | 0.5423    |
123     | 0.5994    | 0.5406    |
124     | 0.5999    | 0.5404    |
125     | 0.5996    | 0.54      |
126     | 0.6       | 0.5401    |
127     | 0.5999    | 0.5403    |
128     | 0.6002    | 0.5407    |
129     | 0.6006    | 0.54      |
130     | 0.6003    | 0.5416    |
131     | 0.6006    | 0.5412    |
132     | 0.6002    | 0.5396    |
133     | 0.6003    | 0.5436    |
134     | 0.6005    | 0.5443    |
135     | 0.6008    | 0.5435    |
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.5322
NDCG@10 on validation data: 0.5477
---------------------------------
ERR@10 on test data: 0.0983

Model saved to: ../results/ranklib_randomforest_model
